{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from dataset import DatasetCIFAR10\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifier parameters.\n",
    "CLASSIFIER_NUMBER_OF_CLASSES = 10\n",
    "CLASSIFIER_NUMBER_OF_EPOCHS = 50\n",
    "CLASSIFIER_LEARNING_RATE = 0.01\n",
    "CLASSIFIER_BATCH_SIZE = 64\n",
    "\n",
    "# Parameters for both agents.\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 5e4\n",
    "PRIOROTIZED_REPLAY_EXPONENT = 3\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "TARGET_COPY_FACTOR = 0.01\n",
    "BIAS_INITIALIZATION = 0\n",
    "\n",
    "NUMBER_OF_STATE_DATA = 1000\n",
    "TRAIN_DATASET_LENGTH = 5000\n",
    "\n",
    "# BatchAgent's parameters.\n",
    "\n",
    "DIRNAME = './batch_agent/' # The resulting batch_agent of this experiment will be written in a file.\n",
    "\n",
    "WARM_START_EPISODES_BATCH_AGENT = 5\n",
    "NN_UPDATES_PER_EPOCHS_BATCH_AGENT = 50\n",
    "\n",
    "TRAINING_EPOCHS_BATCH_AGENT = 5\n",
    "TRAINING_EPISODES_PER_EPOCH_BATCH_AGENT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Delete following directories if they exist.\n",
    "for directory in [cwd+'/__pycache__', cwd+'/wandb', cwd+'/batch_agent', cwd+'/libact', cwd+'/AL_results', cwd+'/checkpoints', cwd+'/summaries']:\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "dataset = DatasetCIFAR10(number_of_state_data=NUMBER_OF_STATE_DATA, train_dataset_length=TRAIN_DATASET_LENGTH)\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(dataset.train_data).float(), torch.tensor(dataset.train_labels).long()), batch_size=CLASSIFIER_BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(torch.tensor(dataset.test_data).float(), torch.tensor(dataset.test_labels).long()), batch_size=CLASSIFIER_BATCH_SIZE, shuffle=False)\n",
    "print(\"Train data are {}.\".format(len(dataset.train_data)))\n",
    "print(\"State data are {}.\".format(len(dataset.state_data)))\n",
    "print(\"Test data are {}.\".format(len(dataset.test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model.\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.resnet18 = models.resnet18(pretrained=True)\n",
    "        for param in self.resnet18.parameters():\n",
    "            param.requires_grad = False\n",
    "        num_ftrs = self.resnet18.fc.in_features\n",
    "        self.resnet18.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 3, 32, 32)\n",
    "        return self.resnet18(x)\n",
    "\n",
    "# Initialize the model and device.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "classifier = CNNClassifier()\n",
    "classifier.to(device)\n",
    "\n",
    "# Define the loss function and optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss = 0\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        true_positive = (predicted == labels).sum().item()\n",
    "        true_positives += true_positive\n",
    "        false_positive = (predicted == labels).sum().item() - true_positive\n",
    "        false_positives += false_positive\n",
    "\n",
    "TARGET_PRECISION = true_positives / (true_positives + false_positives)\n",
    "\n",
    "print(f'Test Loss: {test_loss / len(test_loader)}')\n",
    "print(f'Test Precision: {TARGET_PRECISION:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_envs import LalEnvFirstAccuracy\n",
    "batch_env = LalEnvFirstAccuracy(dataset, classifier, epochs=CLASSIFIER_NUMBER_OF_EPOCHS, classifier_batch_size=CLASSIFIER_BATCH_SIZE, target_precision=TARGET_PRECISION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_helpers import ReplayBuffer\n",
    "replay_buffer = ReplayBuffer(buffer_size=REPLAY_BUFFER_SIZE, prior_exp=PRIOROTIZED_REPLAY_EXPONENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARM-START EPISODES.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the variables.\n",
    "episode_durations = []\n",
    "episode_scores = []\n",
    "episode_number = 1\n",
    "episode_losses = []\n",
    "episode_precisions = []\n",
    "batches = []\n",
    "\n",
    "# Warm start procedure.\n",
    "for _ in range(WARM_START_EPISODES_BATCH_AGENT):\n",
    "    print(\"Episode {}.\".format(episode_number))\n",
    "    # Reset the environment to start a new episode.\n",
    "    state, next_action, indicies_unknown, reward = batch_env.reset(isBatchAgent=False, target_budget=1.0)\n",
    "    done = False\n",
    "    episode_duration = CLASSIFIER_NUMBER_OF_CLASSES\n",
    "\n",
    "    # Before we reach a terminal state, make steps.\n",
    "    while not done:\n",
    "        # Choose a random action.\n",
    "        batch = torch.randint(0, batch_env.n_actions, (1,)).item()\n",
    "        batches.append(batch)\n",
    "\n",
    "        # Get the numbers from 0 to n_actions.\n",
    "        input_numbers = range(0, batch_env.n_actions)\n",
    "\n",
    "        # Non-repeating using sample() function.\n",
    "        batch_actions_indices = torch.tensor(np.random.choice(input_numbers, batch))\n",
    "        action = batch\n",
    "        print(\"- Step.\")\n",
    "        next_state, next_action, indicies_unknown, reward, done = batch_env.step(batch_actions_indices)\n",
    "\n",
    "        if next_action == []:\n",
    "            next_action.append(torch.tensor([0]))\n",
    "\n",
    "        # Store the transition in the replay buffer.\n",
    "        print(\"- Buffer.\")\n",
    "        replay_buffer.store_transition(state, action, reward, next_state, next_action, done)\n",
    "\n",
    "        # Get ready for the next step.\n",
    "        state = next_state\n",
    "        episode_duration += batch\n",
    "\n",
    "        done = True\n",
    "\n",
    "    # Calculate the final accuracy and precision of the episode.\n",
    "    episode_final_acc = batch_env.return_episode_qualities()     \n",
    "    episode_scores.append(episode_final_acc[-1])\n",
    "    episode_final_precision = batch_env.return_episode_precisions()     \n",
    "    episode_precisions.append(episode_final_precision[-1])    \n",
    "    episode_durations.append(episode_duration)  \n",
    "    episode_number += 1\n",
    "\n",
    "# Compute the average episode duration of episodes generated during the warm start procedure.\n",
    "av_episode_duration = np.mean(episode_durations)\n",
    "BIAS_INITIALIZATION = -av_episode_duration / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Plot total budget size per episode.\n",
    "xpoints = torch.tensor(range(0, len(episode_durations)))\n",
    "ypoints = torch.tensor(episode_durations)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(xpoints, ypoints, 'o', color='m')  # Plot points as blue circles.\n",
    "xnew = torch.linspace(xpoints.min(), xpoints.max(), 500)\n",
    "spl = interp1d(xpoints, ypoints, kind='cubic')\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth, color='m')\n",
    "plot_label = \"Budget per episode. *Size of unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc='left')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Budget size (percentage of the UD)\")\n",
    "\n",
    "# Plot total budget size (percentage of the UD) per episode.\n",
    "xpoints = torch.tensor(range(0, len(episode_durations)))\n",
    "ypoints = torch.tensor([x/len(dataset.train_data) for x in episode_durations])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(xpoints, ypoints, 'o', color='k')  # Plot points as blue circles.\n",
    "xnew = torch.linspace(xpoints.min(), xpoints.max(), 500)\n",
    "spl = interp1d(xpoints, ypoints, kind='cubic')\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth, color='k')\n",
    "plot_label = \"Budget per episode. *Size of unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc='left')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Budget size (percentage of the UD)\")\n",
    "\n",
    "# Plot final achieved accuracy per episode.\n",
    "xpoints = torch.tensor(range(0, len(episode_scores)))\n",
    "ypoints = torch.tensor(episode_scores)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(xpoints, ypoints, 'o', color='c')  # Plot points as blue circles.\n",
    "xnew = torch.linspace(xpoints.min(), xpoints.max(), 500)\n",
    "spl = interp1d(xpoints, ypoints, kind='cubic')\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth, color='c')\n",
    "plt.title(\"Final achieved accuracy per episode\", loc='left')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"ACC\")\n",
    "legend_label = \"Maximum ACC: \" + str(max(episode_scores))[:4]\n",
    "plt.legend([legend_label])\n",
    "\n",
    "# Plot final achieved precision per episode.\n",
    "xpoints = torch.tensor(range(0, len(episode_precisions)))\n",
    "ypoints = torch.tensor(episode_precisions)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(xpoints, ypoints, 'o', color='y')  # Plot points as blue circles.\n",
    "xnew = torch.linspace(xpoints.min(), xpoints.max(), 500)\n",
    "spl = interp1d(xpoints, ypoints, kind='cubic')\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth, color='y')\n",
    "plt.title(\"Final achieved precision per episode\", loc='left')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Precision\")\n",
    "legend_label = \"Maximum precision: \" + str(max(episode_precisions))[:4]\n",
    "plt.legend([legend_label])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert the list to a PyTorch tensor.\n",
    "episode_precisions = torch.tensor(episode_precisions)\n",
    "max_precision = torch.max(episode_precisions)\n",
    "\n",
    "# Initialize an empty list to store the warm start batches.\n",
    "warm_start_batches = []\n",
    "\n",
    "# Iterate over the episode precisions and durations.\n",
    "i = 0\n",
    "for precision in episode_precisions:\n",
    "    # Check if the precision is greater than or equal to the maximum precision.\n",
    "    if precision >= max_precision:\n",
    "        # Add the corresponding episode duration to the warm start batches list.\n",
    "        warm_start_batches.append(episode_durations[i])\n",
    "    i += 1\n",
    "\n",
    "# Calculate the target budget\n",
    "TARGET_BUDGET = torch.min(torch.tensor(warm_start_batches)) / len(dataset.train_data)\n",
    "print(\"Target budget is {}.\".format(TARGET_BUDGET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the train dataset length.\n",
    "TRAIN_DATASET_LENGTH = 50000\n",
    "\n",
    "# Create a DatasetCIFAR10 instance with the specified number of state data and train dataset length.\n",
    "dataset = DatasetCIFAR10(number_of_state_data=NUMBER_OF_STATE_DATA, train_dataset_length=torch.tensor(TRAIN_DATASET_LENGTH).long())\n",
    "print(\"Train data are {}.\".format(len(dataset.train_data)))\n",
    "print(\"State data are {}.\".format(len(dataset.state_data)))\n",
    "print(\"Test data are {}.\".format(len(dataset.test_data)))\n",
    "\n",
    "# Create a LalEnvFirstAccuracy instance with the dataset, classifier, and specified epochs and batch size.\n",
    "batch_env = LalEnvFirstAccuracy(dataset, classifier, epochs=CLASSIFIER_NUMBER_OF_EPOCHS, classifier_batch_size=CLASSIFIER_BATCH_SIZE, target_precision=torch.tensor(TARGET_PRECISION).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_dqn import DQN\n",
    "batch_agent = DQN(\n",
    "            observation_length=NUMBER_OF_STATE_DATA,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            target_copy_factor=TARGET_COPY_FACTOR,\n",
    "            bias_average=BIAS_INITIALIZATION,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for update in range(NN_UPDATES_PER_EPOCHS_BATCH_AGENT):\n",
    "    print(\"Update:\", update+1)\n",
    "    minibatch = replay_buffer.sample_minibatch(BATCH_SIZE)\n",
    "    td_error = batch_agent.train(minibatch)\n",
    "    replay_buffer.update_td_errors(td_error, minibatch.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH-AGENT TRAINING.\n",
    "\n",
    "# Initialize the agent.\n",
    "agent_epoch_durations = []\n",
    "agent_epoch_scores = []\n",
    "agent_epoch_precisions = []\n",
    "\n",
    "for epoch in range(TRAINING_EPOCHS_BATCH_AGENT):\n",
    "    print(\"Training epoch {}.\".format(epoch+1))\n",
    "\n",
    "    # Simulate training episodes.\n",
    "    agent_episode_durations = []\n",
    "    agent_episode_scores = []\n",
    "    agent_episode_precisions = []\n",
    "\n",
    "    for training_episode in range(TRAINING_EPISODES_PER_EPOCH_BATCH_AGENT):\n",
    "        # Reset the environment to start a new episode.\n",
    "        state, action_batch, action_unlabeled_data, reward = batch_env.reset(isBatchAgent=True, target_budget=TARGET_BUDGET)\n",
    "        done = False\n",
    "        episode_duration = CLASSIFIER_NUMBER_OF_CLASSES\n",
    "        first_batch = True\n",
    "\n",
    "        # Run an episode.\n",
    "        while not done:\n",
    "            if first_batch:\n",
    "                next_batch = action_batch\n",
    "                next_unlabeled_data = action_unlabeled_data\n",
    "                first_batch = False\n",
    "            else:\n",
    "                next_batch = next_action_batch_size\n",
    "                next_unlabeled_data = next_action_unlabeled_data\n",
    "\n",
    "            selected_batch, selected_indices = batch_agent.get_action(dataset=dataset, model=classifier, state=state, next_action_batch=next_batch, next_action_unlabeled_data=next_unlabeled_data)\n",
    "            next_state, next_action_batch_size, next_action_unlabeled_data, reward, done = batch_env.step(selected_indices)\n",
    "            if next_action_batch_size==[]:\n",
    "                next_action_batch_size.append(np.array([0]))\n",
    "\n",
    "            replay_buffer.store_transition(state, selected_batch, reward, next_state, next_action_batch_size, done)\n",
    "        \n",
    "            # Change the state of the environment.\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "            episode_duration += selected_batch\n",
    "            print(\"- Selected batch is {}.\".format(selected_batch))\n",
    "\n",
    "        agent_episode_final_acc = batch_env.return_episode_qualities()\n",
    "        agent_episode_scores.append(agent_episode_final_acc[-1])\n",
    "        agent_episode_final_precision = batch_env.return_episode_precisions()\n",
    "        agent_episode_precisions.append(agent_episode_final_precision[-1])\n",
    "        agent_episode_durations.append(episode_duration)\n",
    "        \n",
    "    maximum_epoch_precision = max(agent_episode_precisions)\n",
    "    minimum_batches_for_the_maximum_epoch_precision = []\n",
    "    accuracy_for_the_maximum_epoch_precision = []\n",
    "    for i in range(len(agent_episode_precisions)):\n",
    "        if agent_episode_precisions[i] == maximum_epoch_precision:\n",
    "            minimum_batches_for_the_maximum_epoch_precision.append(agent_episode_durations[i])\n",
    "            accuracy_for_the_maximum_epoch_precision.append(agent_episode_scores[i])\n",
    "    agent_epoch_precisions.append(maximum_epoch_precision)\n",
    "    agent_epoch_scores.append(accuracy_for_the_maximum_epoch_precision)\n",
    "    agent_epoch_durations.append(min(minimum_batches_for_the_maximum_epoch_precision))\n",
    "\n",
    "    # NEURAL NETWORK UPDATES.\n",
    "    for update in range(NN_UPDATES_PER_EPOCHS_BATCH_AGENT):\n",
    "        minibatch = replay_buffer.sample_minibatch(BATCH_SIZE)\n",
    "        td_error = batch_agent.train(minibatch)\n",
    "        replay_buffer.update_td_errors(td_error, minibatch.indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
