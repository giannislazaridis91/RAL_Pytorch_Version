{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazaioan/anaconda3/envs/RLA/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lazaioan/anaconda3/envs/RLA/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lazaioan/anaconda3/envs/RLA/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lazaioan/anaconda3/envs/RLA/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lazaioan/anaconda3/envs/RLA/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lazaioan/anaconda3/envs/RLA/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/lazaioan/anaconda3/envs/RLA/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lazaioan/anaconda3/envs/RLA/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lazaioan/anaconda3/envs/RLA/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lazaioan/anaconda3/envs/RLA/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lazaioan/anaconda3/envs/RLA/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lazaioan/anaconda3/envs/RLA/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Depending on the classification model use, we might need to import other packages.\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datasets import DatasetUCI\n",
    "from envs import LalEnvFirstAccuracy\n",
    "from helpers import Minibatch, ReplayBuffer\n",
    "from dqn import DQN\n",
    "from Test_AL import policy_rl\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from scipy.interpolate import make_interp_spline, BSpline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for dataset and model.\n",
    "\n",
    "- australian: 690\n",
    "- breast_cancer: 263\n",
    "- diabetis: 768\n",
    "- flare_solar: 144\n",
    "- german: 1000\n",
    "- heart: 270\n",
    "- mushrooms: 8124\n",
    "- waveform: 5000\n",
    "- wdbc: 569"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_STATE_ESTIMATION = 30\n",
    "SIZE = -1\n",
    "SUBSET = -1 # -1 for using all data points, 0 for even, 1 for odd.\n",
    "N_JOBS = 1 # Can set more if we want to parallelise.\n",
    "# Remove the dataset that will be used for testing.\n",
    "# ['australian', 'breast_cancer', 'diabetis', 'flare_solar', 'german', 'heart', 'mushrooms', 'waveform', 'wdbc']\n",
    "# possible_dataset_names = ['breast_cancer', 'diabetis', 'flare_solar', 'german', 'heart', 'mushrooms', 'waveform', 'wdbc']\n",
    "possible_dataset_names = ['mushrooms']\n",
    "test_dataset_names = ['waveform']\n",
    "# The quality is measured according to a given quality measure \"quality_method\". \n",
    "QUALITY_METHOD = metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() # Find current directory.\n",
    "\n",
    "# Delete following directories if they exist.\n",
    "shutil.rmtree(cwd+'/__pycache__', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/agents', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/AL_results', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/checkpoints', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/summaries', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/Output images', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise a dataset that will contain a sample of datapoint from one the indicated classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = DatasetUCI(possible_dataset_names, n_state_estimation=N_STATE_ESTIMATION, subset=SUBSET, size=SIZE)\n",
    "# If we want to measure test error along with training.\n",
    "dataset_test = DatasetUCI(test_dataset_names, n_state_estimation=N_STATE_ESTIMATION, subset=SUBSET, size=SIZE)\n",
    "# dataset_test: Diabetis datasets consists of 768 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LalEnv init\n",
      "\n",
      "\n",
      "self.dataset\n",
      "<datasets.DatasetUCI object at 0x7f1f6f3ffc88>\n",
      "\n",
      "\n",
      "self.model\n",
      "LogisticRegression(n_jobs=1)\n",
      "\n",
      "\n",
      "self.quality_method\n",
      "<function accuracy_score at 0x7f1f7088fe18>\n",
      "\n",
      "\n",
      "LalEnv init\n",
      "\n",
      "\n",
      "self.dataset\n",
      "<datasets.DatasetUCI object at 0x7f1f6f3ffda0>\n",
      "\n",
      "\n",
      "self.model\n",
      "LogisticRegression(n_jobs=1)\n",
      "\n",
      "\n",
      "self.quality_method\n",
      "<function accuracy_score at 0x7f1f7088fe18>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = LalEnvFirstAccuracy(dataset, model, quality_method=QUALITY_METHOD)\n",
    "env_test = LalEnvFirstAccuracy(dataset_test, model, quality_method=QUALITY_METHOD)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for training RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIRNAME = './agents/' # The resulting agent of this experiment will be written in a file.\n",
    "\n",
    "# Replay buffer parameters.\n",
    "REPLAY_BUFFER_SIZE = 1e4\n",
    "PRIOROTIZED_REPLAY_EXPONENT = 3\n",
    "\n",
    "# Agent parameters.\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "TARGET_COPY_FACTOR = 0.01\n",
    "BIAS_INITIALIZATION = 0 # Default 0 # will be set to minus half of average duration during warm start experiments.\n",
    "\n",
    "# Warm start parameters.\n",
    "WARM_START_EPISODES = 5 # Reduce for test.\n",
    "NN_UPDATES_PER_WARM_START = 100\n",
    "\n",
    "# Episode simulation parameters.\n",
    "EPSILON_START = 1\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_STEPS = 1000\n",
    "\n",
    "# Training parameters.\n",
    "TRAINING_EPOCHS = 5 # Reduce for test.\n",
    "TRAINING_EPISODES_PER_EPOCH = 2 # At each training iteration x episodes are simulated.\n",
    "NN_UPDATES_PER_EPOCHS = 2 # At each training iteration x gradient steps are made.\n",
    "\n",
    "# Validation and test parameters.\n",
    "VALIDATION_EPISODES = 10 # Reduce for test.\n",
    "TESTING_EPISODES = 10 # Reduce for test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size=REPLAY_BUFFER_SIZE, prior_exp=PRIOROTIZED_REPLAY_EXPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Warm-start the replay buffer with random episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1.\n",
      "envs, def reset(self, n_start=2):\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3395a4d55802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# The state value contains vector representation of state of the environment (depends on the classifier).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# The next_action contains vector representations of all actions available to be taken at the next step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mepisode_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lazaioan/Desktop/LAL-RLs/LAL-RL-7/envs.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, n_start)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLalEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mcurrent_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lazaioan/Desktop/LAL-RLs/LAL-RL-7/envs.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, n_start)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_unknown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_known\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_known\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_known\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_bank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_bank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_unknown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Keep track of episode duration to compute average.\n",
    "episode_durations = []\n",
    "episode_scores = []\n",
    "episode_number = 1\n",
    "\n",
    "for _ in range(WARM_START_EPISODES):\n",
    "    \n",
    "    print(\"Episode {}.\".format(episode_number))\n",
    "    # Reset the environment to start a new episode.\n",
    "    # The state value contains vector representation of state of the environment (depends on the classifier).\n",
    "    # The next_action contains vector representations of all actions available to be taken at the next step.\n",
    "    state, next_action, reward = env.reset()\n",
    "    done = False\n",
    "    episode_duration = 2\n",
    "\n",
    "    # Before we reach a terminal state, make steps.\n",
    "    while not done:\n",
    "\n",
    "        # Choose a random action.\n",
    "        batch = random.choice(next_action)[0]\n",
    "        print(\"WARM-STARM, batch:\", batch)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Getting numbers from 0 to n_actions.\n",
    "        inputNumbers =range(0,env.n_actions)\n",
    "\n",
    "        # Non-repeating using sample() function.\n",
    "        batch_actions_indices = np.array(random.sample(inputNumbers, batch))\n",
    "        print(\"WARM-STARM, batch_actions_indices:\", batch_actions_indices)\n",
    "        print(\"WARM-STARM, batch_actions_indices length:\", len(batch_actions_indices))\n",
    "        print(\"\\n\")\n",
    "        action = batch\n",
    "        next_state, next_action, reward, done = env.step(batch_actions_indices)\n",
    "\n",
    "        # Store the transition in the replay buffer.\n",
    "        \"\"\" print(\"BUFFER\")\n",
    "        print(\"state\",state)\n",
    "        print(\"action\", action)\n",
    "        print(\"reward\",reward)\n",
    "        print(\"next_state\", next_state)\n",
    "        print(\"next_action\", next_action)\n",
    "        print(\"done\", done)\n",
    "        print(\"\\n\")\n",
    "        print(\"REPLAY BUFFER STORE TRANSITION\") \"\"\"\n",
    "        print(\"\\n\")\n",
    "        print(\"REPLAY BUFFER STORE TRANSITION\")\n",
    "        replay_buffer.store_transition(state, action, reward, next_state, next_action, done)\n",
    "\n",
    "        # Get ready for next step.\n",
    "        state = next_state\n",
    "        episode_duration += batch\n",
    "    print(\"TOTAL BUDGET\", episode_duration)\n",
    "    \n",
    "    episode_final_acc = env.return_episode_qualities()\n",
    "    episode_scores.append(episode_final_acc[-1])\n",
    "    print(\"Final ACC\", episode_final_acc[-1])   \n",
    "    episode_durations.append(episode_duration)\n",
    "    episode_number+=1\n",
    "\n",
    "# Compute the average episode duration of episodes generated during the warm start procedure.\n",
    "av_episode_duration = np.mean(episode_durations)\n",
    "BIAS_INITIALIZATION = -av_episode_duration/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for warm-start episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total budget size per episode.\n",
    "# Total number of episodes: 100.\n",
    "xpoints = np.array(range(0,len(episode_durations)))\n",
    "ypoints = np.array(episode_durations)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(xpoints, ypoints)\n",
    "plot_label = \"Budget per episode. *Size of unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Budget size (percentage of the UD)\")\n",
    "\n",
    "# Plot total budget size per episode.\n",
    "# Total number of episodes: 100.\n",
    "xpoints = np.array(range(0,len(episode_durations)))\n",
    "ypoints = np.array([x/len(dataset.train_data) for x in episode_durations])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(xpoints, ypoints)\n",
    "plot_label = \"Budget per episode. *Size of unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Budget size (percentage of the UD)\")\n",
    "\n",
    "# Plot final achieved accuracy per episode.\n",
    "# Total number of episodes: 100.\n",
    "xpoints = np.array(range(0,len(episode_scores)))\n",
    "ypoints = np.array(episode_scores)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(xpoints, ypoints)\n",
    "plt.title(\"Final achieved accuracy per episode\", loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"ACC\")\n",
    "legend_label = \"Maximum ACC: \" + str(max(episode_scores))[:4]\n",
    "plt.legend([legend_label]) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = DQN(experiment_dir=DIRNAME,\n",
    "            observation_length=N_STATE_ESTIMATION,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            target_copy_factor=TARGET_COPY_FACTOR,\n",
    "            bias_average=BIAS_INITIALIZATION,\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do updates of the network based on warm start episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for update in range(NN_UPDATES_PER_WARM_START):\n",
    "\n",
    "    print(\"Update:\", update)\n",
    "    \n",
    "    # Sample a batch from the replay buffer proportionally to the probability of sampling.\n",
    "    minibatch = replay_buffer.sample_minibatch(BATCH_SIZE)\n",
    "\n",
    "    # Use batch to train an agent. Keep track of temporal difference errors during training.\n",
    "    td_error = agent.train(minibatch)\n",
    "\n",
    "    # Update probabilities of sampling each datapoint proportionally to the error.\n",
    "    replay_buffer.update_td_errors(td_error, minibatch.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run multiple training iterations. Each iteration consists of:\n",
    "- Generating episodes following agent's actions with exploration.\n",
    "- Validation and test episodes for evaluating performance.\n",
    "- Q-network updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_episode_scores_training = []\n",
    "final_episode_durations_training = []\n",
    "\n",
    "for epoch in range(TRAINING_EPOCHS):\n",
    "\n",
    "    print(\"EPOCH {}.\".format(epoch+1))\n",
    "    \n",
    "    # Compute epsilon value according to the schedule.\n",
    "    epsilon = max(EPSILON_END, EPSILON_START-epoch*(EPSILON_START-EPSILON_END)/EPSILON_STEPS)\n",
    "\n",
    "    # Simulate training episodes.\n",
    "    episode_scores_training = []\n",
    "    episode_durations_training = []\n",
    "\n",
    "    for training_episode in range(TRAINING_EPISODES_PER_EPOCH):\n",
    "        \n",
    "        print(\"Training episode:\", training_episode+1)\n",
    "        # Reset the environment to start a new episode.\n",
    "        state, next_action, reward = env.reset()\n",
    "        print(\"state\", state)\n",
    "        print(\"length state\", len(state))\n",
    "        print(\"length next_action\", len(next_action))\n",
    "        print(\"indices known\", len(env.indices_known), env.indices_known)\n",
    "        print(\"indices unknown\", len(env.indices_unknown), env.indices_unknown)\n",
    "        print(\"n_actions\", env.n_actions)\n",
    "        done = False\n",
    "        episode_duration = 2\n",
    "        \n",
    "        # Run an episode.\n",
    "        while not done:\n",
    "            batch = agent.get_action(state, next_action)\n",
    "            batch = int(next_action[action])\n",
    "            if np.random.ranf() < epsilon:\n",
    "                batch = random.choice(next_action)[0]\n",
    "            print(\"batch\", batch)\n",
    "            inputNumbers =range(0,env.n_actions)\n",
    "            batch_actions_indices = np.array(random.sample(inputNumbers, batch))\n",
    "            print(batch_actions_indices)\n",
    "            # Make another step.\n",
    "            next_state, next_action, reward, done = env.step(batch_actions_indices)\n",
    "\n",
    "            # Store a step in replay buffer.\n",
    "            action = batch\n",
    "            replay_buffer.store_transition(state, action, reward, next_state, next_action, done)\n",
    "            \n",
    "            # Change a state of environment.\n",
    "            state = next_state\n",
    "            episode_duration += batch\n",
    "        epoch_episode = epoch.__str__() + \".\" + training_episode.__str__()\n",
    "        episode_accuracies_training = env.return_episode_qualities()\n",
    "        episode_scores_training.append(episode_accuracies_training[-1])\n",
    "        episode_durations_training.append(episode_duration)\n",
    "    final_episode_durations_training.append(episode_durations_training)\n",
    "    final_episode_scores_training.append(episode_scores_training)\n",
    "\n",
    "    # NEURAL NETWORK UPDATES.\n",
    "    for _ in range(NN_UPDATES_PER_EPOCHS):\n",
    "        minibatch = replay_buffer.sample_minibatch(BATCH_SIZE)\n",
    "        td_error = agent.train(minibatch)\n",
    "        replay_buffer.update_td_errors(td_error, minibatch.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create folder to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder to store the results from the experiments.\n",
    "results_path = r'./Output images' \n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum accuracy per validation epoch and their respective budgets.\n",
    "budgets = []\n",
    "max_scores = []\n",
    "max_score = max(final_episode_scores_training[0])\n",
    "budgets_for_max_scores = []\n",
    "budgets_for_max_scores_help = []\n",
    "for i in range(TRAINING_EPOCHS):\n",
    "    max_scores.append(max(final_episode_scores_training[i]))\n",
    "    for j in range(TRAINING_EPISODES_PER_EPOCH):\n",
    "        if final_episode_scores_training[i][j]==max(final_episode_scores_training[i]):\n",
    "            budgets_for_max_scores_help.append(final_episode_durations_training[i][j])\n",
    "    budgets_for_max_scores.append(np.array(budgets_for_max_scores_help).min())\n",
    "    budgets_for_max_scores_help = []\n",
    "    if max(final_episode_scores_training[i])>max_score:\n",
    "        max_score = max(final_episode_scores_training[i])\n",
    "for i in range(TRAINING_EPOCHS):\n",
    "    for j in range(TRAINING_EPISODES_PER_EPOCH):\n",
    "        if final_episode_scores_training[i][j]==max_score:\n",
    "            budgets.append(final_episode_durations_training[i][j])\n",
    "print(\"The maximum accuracy is {}.\".format(max_score))\n",
    "print(\"The budget for the maximum accuracy is {}.\".format(np.array(budgets).min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot maximum achieved accuracy per epoch and the respective budgets.\n",
    "# Total number of iterations (epochs): 1000.\n",
    "# Total number of episodes per epoch: 10.\n",
    "\n",
    "xpoints = np.array(range(1,len(budgets_for_max_scores)+1))\n",
    "ypoints = np.array(budgets_for_max_scores)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(xpoints, ypoints, color='m')\n",
    "plot_label = \"Budget per max ACC. | Unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Budget\")\n",
    "plt.savefig(\"Output images/TRAINING, Budget.png\")\n",
    "\n",
    "xpoints = np.array(range(1,len(budgets_for_max_scores)+1))\n",
    "ypoints = np.array([x/len(dataset.train_data) for x in budgets_for_max_scores])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(xpoints, ypoints, color='k')\n",
    "plot_label = \"Budget (percentage of the UD) per max ACC. | Unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Budget\")\n",
    "#plt.savefig(\"Output images/TRAINING, Budget percentage.png\")\n",
    "\n",
    "xpoints = np.array(range(1,len(max_scores)+1))\n",
    "ypoints = np.array(max_scores)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(xpoints, ypoints, color='c')\n",
    "plt.title(\"Max ACC per epoch\", loc = \"left\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Max ACC\")\n",
    "legend_1 = \"Maximum ACC: \" + str(max_score)[:4]\n",
    "legend_2 = \", \"\n",
    "legend_3 = \"Budget: \" + str(np.array(budgets).min())[:4]\n",
    "plt.legend([legend_1 + legend_2 + legend_3])\n",
    "#plt.savefig(\"Output images/TRAINING, Max ACC.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test episodes are run. Use env_test for it.\n",
    "episode_scores_testing = []\n",
    "episode_durations_testing = []\n",
    "final_episode_scores_testing = []\n",
    "final_episode_durations_testing = []\n",
    "for testing_episode in range(TESTING_EPISODES):\n",
    "    print(\"Testing episode:\", testing_episode)\n",
    "    episode_duration = 2\n",
    "    done = False\n",
    "    state, next_action, reward = env_test.reset()\n",
    "    print(\"next_action\",next_action)\n",
    "    print(\"state\", state)\n",
    "    while not(done):\n",
    "        batch = agent.get_action(state, next_action)\n",
    "        batch = int(next_action[batch])\n",
    "        print(\"batch\",batch)\n",
    "        inputNumbers =range(0,env.n_actions)\n",
    "        print(inputNumbers)\n",
    "        batch_actions_indices = np.array(random.sample(inputNumbers, batch))\n",
    "        print(\"batch_actions_indices\",batch_actions_indices)\n",
    "        next_state, next_action, reward, done = env_test.step(batch_actions_indices)\n",
    "        state = next_state\n",
    "        episode_duration += batch\n",
    "    episode_accuracies_testing = env_test.return_episode_qualities()\n",
    "    episode_scores_testing.append(episode_accuracies_testing[-1])\n",
    "    episode_durations_testing.append(episode_duration)\n",
    "final_episode_scores_testing.append(episode_scores_testing)\n",
    "final_episode_durations_testing.append(episode_durations_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for testing episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum accuracy per validation epoch and their respective budgets.\n",
    "budgets = []\n",
    "max_score = max(final_episode_scores_testing[0])\n",
    "for i in range(len(final_episode_scores_testing[0])):\n",
    "    if final_episode_scores_testing[0][i]==max_score:\n",
    "        budgets.append(final_episode_durations_testing[0][i])\n",
    "print(\"The maximum accuracy is {}.\".format(max_score))\n",
    "print(\"The budget for the maximum accuracy is {}.\".format(np.array(budgets).min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot maximum achieved accuracy per testing iteration and the respective budgets.\n",
    "# Total number of iterations (epochs): 10.\n",
    "# Total number of episodes per iteration: 500.\n",
    "\n",
    "xpoints = np.array(range(1,len(final_episode_durations_testing[0])+1))\n",
    "ypoints = np.array(final_episode_durations_testing[0])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(xpoints, ypoints, color='m')\n",
    "plot_label = \"Budget per max ACC. | Unlabeled data: \" + str(len(dataset.test_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean budget\")\n",
    "\n",
    "xpoints = np.array(range(1,len(final_episode_durations_testing[0])+1))\n",
    "ypoints = np.array([x/len(dataset.test_data) for x in final_episode_durations_testing[0]])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(xpoints, ypoints, color='k')\n",
    "plot_label = \"Budget (percentage of the UD) per max ACC. | Unlabeled data: \" + str(len(dataset.test_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Mean budget\")\n",
    "\n",
    "xpoints = np.array(range(1,len(final_episode_scores_testing[0])+1))\n",
    "ypoints = np.array(final_episode_scores_testing[0])\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(xpoints, ypoints, color='c')\n",
    "plt.title(\"Max ACC\", loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Max ACC\")\n",
    "legend_1 = \"Maximum ACC: \" + str(np.array(final_episode_scores_testing[0]).max())[:4]\n",
    "legend_2 = \", \"\n",
    "legend_3 = \"Budget: \" + str(np.array(budgets).min())[:4]\n",
    "plt.legend([legend_1 + legend_2 + legend_3])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
